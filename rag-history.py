from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_community.vectorstores import FAISS
from langchain.chains import create_history_aware_retriever
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter
import json

# ================== é…ç½®å‚æ•° ==================
MODEL_NAME = "deepseek-r1:8b"
HISTORY_FILE = "chat_history.json"
HISTORY_WINDOW = 3

# ================== æ–°ç‰ˆPromptæ¨¡æ¿ ==================
DEEPSEEK_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ä¸¥è°¨æŠ€æœ¯æ–‡æ¡£åˆ†æè¦æ±‚ï¼š
1. åˆ†ç‚¹å›ç­”ï¼ˆâ˜…æ ‡è®°ï¼‰
2. æ ‡æ³¨é¡µç ï¼ˆç¤ºä¾‹ï¼šâ˜…å†…å®¹...(p.12)ï¼‰
3. ä½¿ç”¨ä¸­æ–‡ä½œç­”"""),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),  # ç»Ÿä¸€ä½¿ç”¨inputä½œä¸ºè¾“å…¥é”®
])

# ================== ä¿®å¤åçš„RAGç³»ç»Ÿ ==================
class DeepSeekRAGSystem:
    def __init__(self):
        self.vector_db = self.init_vector_db()
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            k=HISTORY_WINDOW,
            return_messages=True
        )
        self.chain = self.build_chain()

    def init_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        loader = DirectoryLoader(
            "./doc", 
            glob="**/*.pdf",
            loader_cls=PyPDFLoader
        )
        texts = RecursiveCharacterTextSplitter(
            chunk_size=1024,
            chunk_overlap=128,
            separators=["\n\n", "ã€‚", "\n"]
        ).split_documents(loader.load())
        
        return FAISS.from_documents(
            texts,
            OllamaEmbeddings(model="nomic-embed-text")
        )

    def build_chain(self):
        """æ„å»ºç¬¦åˆæ–°ç‰ˆAPIçš„é—®ç­”é“¾"""
        retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})
        
        # å†å²æ„ŸçŸ¥æ£€ç´¢å™¨
        history_prompt = ChatPromptTemplate.from_messages([
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
            ("system", "æ ¹æ®å¯¹è¯å†å²é‡æ–°ç»„ç»‡é—®é¢˜")
        ])
        
        history_aware_retriever = create_history_aware_retriever(
            llm=Ollama(model=MODEL_NAME),
            retriever=retriever,
            prompt=history_prompt
        )

        # ä¸»é—®ç­”é“¾
        return (
            RunnablePassthrough.assign(
                context=history_aware_retriever | (lambda docs: "\n\n".join(d.page_content for d in docs))
            )
            | DEEPSEEK_PROMPT
            | Ollama(model=MODEL_NAME, temperature=0.3)
        )
        
    def save_history(self):
        with open(HISTORY_FILE, 'w') as f:
            json.dump(self.memory.load_memory_variables({}), f)

    def load_history(self):
        try:
            with open(HISTORY_FILE, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return []    

    def chat_loop(self):
        """ä¿®å¤åçš„å¯¹è¯å¾ªç¯"""
        print("DeepSeek RAGç³»ç»Ÿå·²å¯åŠ¨ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸï¼‰")
        while True:
            query = input("\nç”¨æˆ·æé—®ï¼š")
            if query.lower() in ['é€€å‡º', 'exit']:
                print("å†è§ï¼")
                break
            
            response = self.chain.invoke({
                "input": query,  # ç»Ÿä¸€ä½¿ç”¨inputä½œä¸ºè¾“å…¥é”®
                "chat_history": self.memory.buffer
            })
            
            self.memory.save_context({"input": query}, {"output": response})
            
            print("\n" + "="*60)
            print(f"ğŸ“Œ é—®é¢˜ï¼š{query}")
            print("ğŸ” å›ç­”ï¼š\n" + response)
            print("="*60)

# ================== ä¸»æ‰§è¡Œ ==================
if __name__ == "__main__":
    rag_system = DeepSeekRAGSystem()
    rag_system.chat_loop()
